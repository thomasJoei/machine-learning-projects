{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center style=\"font-size:26px\"><b>Workbook</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Predicting a product's category based on the review. Here are the possible labels for the product categories:\n",
    "    - Automotive\n",
    "    - Beauty\n",
    "    - Books\n",
    "    - Clothing, Shoes & Jewelry\n",
    "    - Digital Music\n",
    "    - Electronics\n",
    "    - Grocery & Gourmet Food\n",
    "    - Home & Kitchen\n",
    "    - Sports & Outdoors\n",
    "    - Toys & Games\n",
    "    \n",
    "\n",
    "2. Prediction a review rating based on the text(Next session).\n",
    "\n",
    "The objective is to read the Amazon.com product information and reviews dataset, extract features from it, output a training, dev and test set out of it, and complete a classification task. The end goal is to complete both classification and regression task. \n",
    "\n",
    "You're free to use whichever library you want (including the standard library only if you desire so). The following libraries are available to you through this lab:\n",
    "\n",
    "1. Numpy/Scipy\n",
    "2. Pandas\n",
    "3. Scikit-learn\n",
    "4. NLTK\n",
    "\n",
    "I suggest you start by having an end-to-end solution working before starting to think about extracting fancy features. You can always add those at the end if you still have time. I added a few key sections in the notebook already, feel free to follow that structure, or to reorganize it the way you prefer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 35G\r\n",
      "-rw-r--r-- 1 root 146M Oct 25  2018 metadata.csv\r\n",
      "-rw-r--r-- 1 root 721M Feb 17  2016 reviews_Automotive.json\r\n",
      "-rw-r--r-- 1 root 1.1G Feb 17  2016 reviews_Beauty.json\r\n",
      "-rw-r--r-- 1 root  19G Feb 18  2016 reviews_Books.json\r\n",
      "-rw-r--r-- 1 root 2.8G Feb 18  2016 reviews_Clothing_Shoes_and_Jewelry.json\r\n",
      "-rw-r--r-- 1 root 523M Feb 18  2016 reviews_Digital_Music.json\r\n",
      "-rw-r--r-- 1 root 5.1G Feb 18  2016 reviews_Electronics.json\r\n",
      "-rw-r--r-- 1 root 700M Feb 18  2016 reviews_Grocery_and_Gourmet_Food.json\r\n",
      "-rw-r--r-- 1 root 2.5G Feb 18  2016 reviews_Home_and_Kitchen.json\r\n",
      "-rw-r--r-- 1 root 1.9G Feb 18  2016 reviews_Sports_and_Outdoors.json\r\n",
      "-rw-r--r-- 1 root 1.3G Feb 18  2016 reviews_Toys_and_Games.json\r\n"
     ]
    }
   ],
   "source": [
    "ll -h /mnt/amazon_dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# reviews_Automotive\n",
    "# reviews_Beauty\n",
    "# reviews_Books\n",
    "# reviews_Clothing_Shoes_and_Jewelry\n",
    "# reviews_Digital_Music\n",
    "# reviews_Electronics\n",
    "# reviews_Grocery_and_Gourmet_Food\n",
    "# reviews_Home_and_Kitchen\n",
    "# reviews_Sports_and_Outdoors\n",
    "# reviews_Toys_and_Games\n",
    "\n",
    "# automotive_data = pd.read_json(\"/mnt/amazon_dataset/reviews_Automotive.json\", lines=True)\n",
    "# beauty_data = pd.read_json(\"/mnt/amazon_dataset/reviews_Beauty.json\", lines=True)\n",
    "# # books_data = pd.read_json(\"/mnt/amazon_dataset/reviews_Books.json\", lines=True)\n",
    "# clothing_data = pd.read_json(\"/mnt/amazon_dataset/reviews_Clothing_Shoes_and_Jewelry.json\", lines=True)\n",
    "# music_data = pd.read_json(\"/mnt/amazon_dataset/reviews_Digital_Music.json\", lines=True)\n",
    "# electronics_data = pd.read_json(\"/mnt/amazon_dataset/reviews_Electronics.json\", lines=True)\n",
    "# grocery_data = pd.read_json(\"/mnt/amazon_dataset/reviews_Grocery_and_Gourmet_Food.json\", lines=True)\n",
    "# homekitchen_data = pd.read_json(\"/mnt/amazon_dataset/reviews_Home_and_Kitchen.json\", lines=True)\n",
    "# sports_data = pd.read_json(\"/mnt/amazon_dataset/reviews_Sports_and_Outdoors.json\", lines=True)\n",
    "# toys_data = pd.read_json(\"/mnt/amazon_dataset/reviews_Toys_and_Games.json\", lines=True)\n",
    "\n",
    "# cellphones = pd.read_json(\"/mnt/amazon_dataset/reviews_Cell_Phones_and_Accessories.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sampleSize = 100_000\n",
    "# sampleSize = 1_000\n",
    "\n",
    "automotive_data = next(iter(pd.read_json(\"/mnt/amazon_dataset/reviews_Automotive.json\", lines=True, chunksize=sampleSize)))\n",
    "beauty_data = next(iter(pd.read_json(\"/mnt/amazon_dataset/reviews_Beauty.json\", lines=True, chunksize=sampleSize)))\n",
    "books_data = next(iter(pd.read_json(\"/mnt/amazon_dataset/reviews_Books.json\", lines=True, chunksize=sampleSize)))\n",
    "clothing_data = next(iter(pd.read_json(\"/mnt/amazon_dataset/reviews_Clothing_Shoes_and_Jewelry.json\", lines=True, chunksize=sampleSize)))\n",
    "music_data = next(iter(pd.read_json(\"/mnt/amazon_dataset/reviews_Digital_Music.json\", lines=True, chunksize=sampleSize)))\n",
    "electronics_data = next(iter(pd.read_json(\"/mnt/amazon_dataset/reviews_Electronics.json\", lines=True, chunksize=sampleSize)))\n",
    "grocery_data = next(iter(pd.read_json(\"/mnt/amazon_dataset/reviews_Grocery_and_Gourmet_Food.json\", lines=True, chunksize=sampleSize)))\n",
    "homekitchen_data = next(iter(pd.read_json(\"/mnt/amazon_dataset/reviews_Home_and_Kitchen.json\", lines=True, chunksize=sampleSize)))\n",
    "sports_data = next(iter(pd.read_json(\"/mnt/amazon_dataset/reviews_Sports_and_Outdoors.json\", lines=True, chunksize=sampleSize)))\n",
    "toys_data = next(iter(pd.read_json(\"/mnt/amazon_dataset/reviews_Toys_and_Games.json\", lines=True, chunksize=sampleSize)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Inspecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !head -n 1 /mnt/amazon_dataset/reviews_Automotive.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Joining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'df' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "def mapToCategory(df, category):\n",
    "    df['roots'] = category\n",
    "    return df\n",
    "\n",
    "df = mapToCategory(automotive_data, \"Automotive\") \\\n",
    ".append(mapToCategory(beauty_data, \"Beauty\")) \\\n",
    ".append(mapToCategory(books_data, \"Books\")) \\\n",
    ".append(mapToCategory(clothing_data, \"Clothing_Shoes_and_Jewelry\")) \\\n",
    ".append(mapToCategory(music_data, \"Digital_Music\")) \\\n",
    ".append(mapToCategory(electronics_data, \"Electronics\")) \\\n",
    ".append(mapToCategory(grocery_data, \"Grocery_and_Gourmet_Food\")) \\\n",
    ".append(mapToCategory(homekitchen_data, \"Home_and_Kitchen\")) \\\n",
    ".append(mapToCategory(sports_data, \"Sports_and_Outdoors\")) \\\n",
    ".append(mapToCategory(toys_data, \"Toys_and_Games\")) \n",
    "\n",
    "df.set_index('asin')\n",
    "%store df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%store -r df\n",
    "df\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cleanedDf = df.drop(columns=['unixReviewTime', 'reviewTime', 'reviewerName', 'reviewerID'])\n",
    "\n",
    "review_text = df[\"reviewText\"].values.tolist()\n",
    "Xall = review_text\n",
    "yall = df[\"roots\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "Xtrain, Xrest, ytrain, yrest = model_selection.train_test_split(Xall, yall, test_size=0.2)\n",
    "Xdev, Xtest, ydev, ytest = model_selection.train_test_split(Xrest, yrest, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.stats.distributions import randint, uniform\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "# vectors = vectorizer.fit_transform(Xtrain)\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "# final = tfidf.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'text_cls_bays' (Pipeline)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.stats.distributions import randint, uniform\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "bayes_classifier = MultinomialNB(alpha=1)\n",
    "text_cls_bayes = Pipeline([('vect', vectorizer),\n",
    "                        ('tfidf', tfidf),\n",
    "                        ('clf', bayes_classifier)])\n",
    "\n",
    "# the pipeline call the 'fit_transform' method of every step\n",
    "\n",
    "text_cls_bayes.fit(Xtrain, ytrain)\n",
    "# vectors\n",
    "%store text_cls_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# from scipy.stats.distributions import randint, uniform\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn import tree\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "# from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "# svm_classifier = SVC(kernel='linear', C = 1.0)\n",
    "# # svm_classifier = GaussianProcessClassifier(1.0 * RBF(1.0), copy_X_train=False)\n",
    "\n",
    "# text_cls_svm = Pipeline([('vect', vectorizer),\n",
    "#                     ('tfidf', tfidf),\n",
    "#                     ('clf', svm_classifier)])\n",
    "\n",
    "# text_cls_svm.fit(Xtrain, ytrain);\n",
    "# %store text_cls_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(vectorizer.get_feature_names())\n",
    "%store -r text_cls_bayes\n",
    "# %store -r text_cls_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Writing the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            precision    recall  f1-score   support\n",
      "\n",
      "                Automotive       0.87      0.64      0.74      9917\n",
      "                    Beauty       0.91      0.83      0.87      9891\n",
      "                     Books       0.98      0.94      0.96      9969\n",
      "Clothing_Shoes_and_Jewelry       0.86      0.78      0.82      9905\n",
      "             Digital_Music       0.97      0.94      0.95     10247\n",
      "               Electronics       0.63      0.98      0.76     20069\n",
      "  Grocery_and_Gourmet_Food       0.94      0.86      0.90     10072\n",
      "          Home_and_Kitchen       0.88      0.81      0.84     10004\n",
      "       Sports_and_Outdoors       0.83      0.47      0.60      9935\n",
      "            Toys_and_Games       0.83      0.81      0.82      9991\n",
      "\n",
      "                  accuracy                           0.82    110000\n",
      "                 macro avg       0.87      0.81      0.83    110000\n",
      "              weighted avg       0.85      0.82      0.82    110000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import sklearn\n",
    "\n",
    "\n",
    "y_test_pred = text_cls_bayes.predict(Xtest)\n",
    "\n",
    "print(sklearn.metrics.classification_report(ytest, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# import sklearn\n",
    "\n",
    "\n",
    "# y_test_pred = text_cls_svm.predict(Xtest)\n",
    "\n",
    "# print(sklearn.metrics.classification_report(ytest, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66032959, 0.0073534 , 0.00789523, 0.01277082, 0.03832244,\n",
       "        0.12120802, 0.00673737, 0.01369093, 0.0563937 , 0.07529851]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cls_bayes.predict_proba([\"car\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"50 cent rap ghetto\" : {'bays': 'Digital_Music'}\n",
      "\"dodge\" : {'bays': 'Automotive'}\n",
      "\"plate number is \" : {'bays': 'Automotive'}\n",
      "\"I like to play with it\" : {'bays': 'Toys_and_Games'}\n",
      "\"Summary is great, the preface looks good\" : {'bays': 'Electronics'}\n"
     ]
    }
   ],
   "source": [
    "def testClassifiers(sentence):\n",
    "    results = {'bayes': text_cls_bayes.predict([sentence])[0]\n",
    "#                'svm': text_cls_svm.predict([sentence])[0]\n",
    "              }\n",
    "    return '\"{}\" : '.format(sentence) + str(results)\n",
    "# print(text_cls_bayes.predict_proba([\"yolo I like that spice !\"]))\n",
    "\n",
    "                                            \n",
    "print(testClassifiers(\"50 cent rap ghetto\"))\n",
    "print(testClassifiers(\"dodge\"))\n",
    "print(testClassifiers(\"plate number is \"))\n",
    "print(testClassifiers(\"I like to play with it\"))\n",
    "print(testClassifiers(\"Summary is great, the preface looks good\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bays_classifier.joblib']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(text_cls_bayes, 'bayes_classifier.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 68644\r\n",
      "-rw-r--r-- 1 tjoeisseint 67569865 May 16 13:25 bays_classifier.joblib\r\n",
      "-rw-r--r-- 1 tjoeisseint  2633694 May 15 20:29 Classification_demo.ipynb\r\n",
      "-rw-r--r-- 1 tjoeisseint    55286 May 16 13:21 Environment.ipynb\r\n",
      "-rw-r--r-- 1 tjoeisseint     7287 May 15 16:34 Welcome.ipynb\r\n",
      "-rw-r--r-- 1 tjoeisseint    18890 May 16 13:26 Workbook.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Benchmark/Troubleshooting/Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook is pid=8009\n",
      "USER        PID %CPU %MEM     TIME\n",
      "509        4517  0.0  0.0 00:00:34\n",
      "509        4779  0.0  0.0 00:00:02\n",
      "509        4792  0.0  0.0 00:00:02\n",
      "509        6012  0.0  0.0 00:00:01\n",
      "509        6031  0.0  0.0 00:00:01\n",
      "509        \u001b[01;31m\u001b[K8009\u001b[m\u001b[K  163  1.0 00:04:35\n",
      "509        8037  0.0  0.0 00:00:00\n",
      "509        8039  0.0  0.0 00:00:00\n",
      "509        8040  0.0  0.0 00:00:00\n",
      "509        8041  0.0  0.0 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Use this snippet to get the current CPU/memory usage of this notebook\n",
    "import os\n",
    "pid=os.getpid(); print(\"This notebook is pid={}\".format(pid))\n",
    "!ps -u `whoami` -o user,pid,%cpu,%mem,time | grep --color -E '^|{pid}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
